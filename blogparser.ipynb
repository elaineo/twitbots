{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO \n",
    "# Set username & blog\n",
    "base_url = 'http://www.nytimes.com/column/paul-krugman'\n",
    "username = 'innocuousegg0'\n",
    "\n",
    "## Absolute path of chromedriver\n",
    "chromedriver = \"/Users/elaine/Documents/twitbots/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For sites that block scrapers\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "# ft alphaville\n",
    "for i in range(1,250):\n",
    "    url = base_url + str(i)\n",
    "    r = requests.get(url)\n",
    "    articles = BeautifulSoup(r.content).findAll('h3')\n",
    "    for art in articles:\n",
    "        try:\n",
    "            href = art.find('a')['href']\n",
    "            urls.append(href)\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blogs = {}\n",
    "# require http referer for FT\n",
    "s = requests.Session()\n",
    "s.headers.update({'referer': 'http://www.google.com/'})\n",
    "\n",
    "for url in urls:\n",
    "    r = s.get(url)\n",
    "    content = BeautifulSoup(r.content).find('div',{'class': 'entry-content'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "## Most wordpress blogs\n",
    "##\n",
    "\n",
    "urls = []\n",
    "blogs = {}\n",
    "s = requests.Session()\n",
    "\n",
    "for i in range(1,91):\n",
    "    url = base_url + str(i)\n",
    "    req = requests.Request('GET', url, \n",
    "                         headers={'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'})\n",
    "    prepped = req.prepare()\n",
    "    r = s.send(prepped)\n",
    "    articles = BeautifulSoup(r.content).findAll('article', {'class': 'post'})\n",
    "    for art in articles:\n",
    "        ent = art.find('h1',{'class': 'entry-title'})\n",
    "        href = ent.find('a')['href']\n",
    "        urls.append(href)\n",
    "        content = art.find('div',{'class': 'entry-content'})\n",
    "        text = ''\n",
    "        p = 0\n",
    "        for t in content.findAll('p'):\n",
    "            if t.get('style') and \"padding-left: 30px\" in t.get('style'):\n",
    "                continue\n",
    "            blogs[href+str(p)] = t.getText()\n",
    "            p+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Older Wordpress blogs\n",
    "\n",
    "urls = []\n",
    "blogs = {}\n",
    "\n",
    "for i in range(1,38):\n",
    "    url = base_url + str(i)\n",
    "    r = requests.get(url)\n",
    "    articles = BeautifulSoup(r.content).findAll('div',{'class': 'post'})\n",
    "    for art in articles:\n",
    "        try:\n",
    "            ent = art.find('h2',{'class': 'posttitle'})\n",
    "            href = ent.find('a')['href']\n",
    "            urls.append(href)\n",
    "            content = art.find('div',{'class': 'entry'})\n",
    "            p = 0\n",
    "            for t in content.findAll('p'):\n",
    "                blogs[href+str(p)] = t.getText()\n",
    "                p+=1\n",
    "        except:\n",
    "            print art\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.nytimes.com/2015/05/11/opinion/paul-krugman-wall-street-vampires.html\n"
     ]
    }
   ],
   "source": [
    "## NYtimes\n",
    "##\n",
    "browser = webdriver.Chrome(chromedriver)\n",
    "\n",
    "url = base_url\n",
    "\n",
    "browser.get(url)\n",
    "time.sleep(1)\n",
    "\n",
    "elem = browser.find_element_by_tag_name(\"body\")\n",
    "button = browser.find_element_by_class_name(\"load-more-button\").click()\n",
    "\n",
    "no_of_pagedowns = 0\n",
    "\n",
    "while no_of_pagedowns:\n",
    "    elem.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.2)\n",
    "    no_of_pagedowns-=1\n",
    "\n",
    "post_elems = browser.find_elements_by_class_name(\"story-link\")\n",
    "\n",
    "urls=[]\n",
    "for post in post_elems:\n",
    "    urlclean = post.get_attribute('href').split('?')\n",
    "    urls.append(urlclean[0])\n",
    "    \n",
    "print urls[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NYtimes continued\n",
    "##\n",
    "\n",
    "blogs = {}\n",
    "\n",
    "for url in urls:    \n",
    "    r = requests.get(url)\n",
    "    content = BeautifulSoup(r.content).find('div',{'class': 'entry-content'})\n",
    "    text=''\n",
    "    try:\n",
    "        for t in content.findAll('p',{'class': 'story-body-text'}):\n",
    "             text += t.getText()\n",
    "        blogs[url] = text\n",
    "    except:\n",
    "        print url\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dump file\n",
    "\n",
    "with open('data/%s-urls.json' % username, 'w') as f:\n",
    "    json.dump(urls, f)\n",
    "with open('data/%s-blog.json' % username, 'w') as f:\n",
    "    json.dump(blogs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
